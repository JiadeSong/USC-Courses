import sklearnfrom sklearn.feature_extraction.text import TfidfVectorizerimport osfrom itertools import groupbyimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedKFoldfrom sklearn.model_selection import cross_val_scorefrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn.svm import SVCfrom sklearn import treefrom sklearn.neural_network import MLPClassifierimport csvfrom sklearn.metrics import mean_squared_error"""At this point I am assuming that:--you are comfortable writing and testing python code for the purposes of these homeworks.--you are comfortable with statistical/ML tools like sklearn and numpy.--if given reasonable problems involving featurization of raw data, training, testing, sampling etc., you can figure out reasonable solutions to these problems.MAX SCORE POSSIBLE: 100 pointsGood luck!""""""[40 points] Replace 'pass' below as appropriate. Make sure to read the notes in the function header.<The main goal of this function is to ensure you can use the 'tfidf' function offered in the sklearnlib: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html>"""def encode_text_tfidf_vectors(in_folder, tfidf_max_df, tfidf_ngram_range): # replace this path with your own after you've unzipped the input file.    """    1. The goal of this function is to take the path to your text file, open the file, read in/encode the text files as tf-idf vectors    and labels and output the tf-idf matrix X and the labels-vector y. You may use the sk-learn tf-idf vectorizer    with default parameters. Initially, the goal should be to get the full system working rather than 'optimize'    the tf-idf and its parameters for best performance.    2. Note that the text in the input files has been 'encoded' (in this case, each word has been assigned to a number.    This makes your job easier, not harder, since it means we won't have to process the 'raw' text and perform NLP-based preprocessing    like tokenization/word segmentation etc.). Make sure to read the readme file to understand what's going on here, including     on how the name of the file tells you whether a file is 'legit' or 'spam'     Hint: A useful package here is https://docs.python.org/2/library/os.html#os.listdir, which comes with python     and allows you to directly list/manipulate files in folders etc. Other ways are fine too, as long as your     approach is programmatic (i.e. you are not allowed to manually input the names of files in this exercise.     3. If subject is false, then that means you should only produce the tf-idf of the 'main body'. If subject is true,     then you should produce the tf-idf of only the subject (see extra credit question at the end of this program).     In no time during this homework will we be producing the     tf-idf of the subject+main body combined.     Hint: You will need to parse the contents of the file accordingly. The package 're' (also included in the python     system lib) is your friend.    """    X = None    y = None    fileslist = os.listdir(in_folder)    y = []    rawdata = []    vectorizer = TfidfVectorizer(ngram_range=tfidf_ngram_range, max_df=tfidf_max_df)    for i in fileslist:        namesplited = [''.join(g) for _, g in groupby(i, str.isalpha)]        filepath = in_folder + '/' + i        f = open(filepath,'r')        lines = f.readlines()        lines1 = [x.replace("\n", "") for x in lines]        rawdata.append(lines1[2])        # print(lines1[2])        if namesplited[1] == 'legit':            y.append(1)        else:             y.append(0)    vector = vectorizer.fit_transform(rawdata)    vocabulary = vectorizer.vocabulary_    IDF = vectorizer.idf_    # print(vocabulary)    # print(IDF)    dense = vector.todense()    feature_names = vectorizer.get_feature_names()    dense_list = dense.tolist()    df = pd.DataFrame(dense_list,columns=feature_names)    X = dense    # print(df)    # print(len(y))    # put your code here. You can define helper functions, but if you do please define them before this function    return X, y"""[40 points] Train a classifier on the train dataset that is able to predict spam. You must:--Try at least five different classifiers--Try at least three different tf-idf 'settings' i.e. try to play with the parameters in the sklearn tf-idf vectorizer to achieveoptimal performance (measured using 'accuracy' i.e. the percentage of correct classifications).--Use some kind of 'validation' mechanism to ensure that you truly select the best classifier and don't overfit. Thereare several ways of achieving this including (i) cross validation (if you are able to do it), or (ii) split your 'original' trainingdata into a 'training' and 'validation' set (e.g., maybe using an 80/20 or 90/10 split). Use the 'training' portionfor training your classifier, and your 'validation' portion for getting an unbiased estimate of classifier performance.Make sure to use stratified sampling (the code from the previous HW is your friend here).You MUST NOT use the test dataset at all for this part of the experiment. The deliverable for this exercise shouldbe both the code that you write  below as well as a short report showing results for the different things you tried. The report can be a spreadsheet, text, word etc. but we must be able to read and understand it. Shorter is better, but not at the cost of readability/clarity. Be liberal in your use of tables/diagrams. For example, you could produce a table with 5 X 3 rows, showing performance on the validation set for each of the fifteen things you tried. [20 points] Using the best and second-best classifiers you got from above, apply them once each on the entire test dataset. If you run into the out-of-vocabulary problem i.e. there are words in your test data that are not in your training data, you can delete the word, although in practice we would be taking a more sophisticated approach. What are the accuracy measures that you are getting for both? Is the difference greater, smaller or equal compared to performance difference on validation set/cross-validation? What would be the accuracy for a classifier that labeled the data 'randomly'? (Hint: for the last question, use a bernoulli distribution i.e. toss a coin, with spam coming up with probability p and non-spam with probability 1-p. Use the number of spam/legit samples in the training data to estimate the ideal value for p.)"""tfidfmaxdfset = [0.5, 0.75]tfidfngramrangeset = [(1, 1), (1, 2)]c = []outputfile = '/Users/jiadesong/Desktop/ISE540/hw3/output.csv'kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)Tree = tree.DecisionTreeClassifier()GBC = GradientBoostingClassifier()LinearSVM = SVC(kernel="linear", C=0.025)RBFSVM = SVC(gamma=2, C=1)NeuralNet = MLPClassifier(alpha=1, max_iter=1000)Models = [Tree, GBC, LinearSVM, RBFSVM, NeuralNet]# with open(outputfile, 'w') as f:#     write = csv.writer(f)#     for j in Models:#         c.append(j)#         for k in range(2):#             for n in range(2):#                 X, y = encode_text_tfidf_vectors('/Users/jiadesong/Desktop/ISE540/hw3/files/train', tfidfmaxdfset[k], tfidfngramrangeset[n])#                 scores = cross_val_score(j, X, y, cv=kfold)#                 c.append(scores.mean())#         write.writerow(['--', 'maxdf=0.5, ngramrange=(1,1)', 'maxdf=0.5, ngramrange=(1,2)', 'maxdf=0.75, ngramrange=(1,1)', 'maxdf=0.75, ngramrange=(1,2)'])#         write.writerow(c)#         c = []"""Answer: According to the output of last problem, the best two classifiers are DecisionTreeClassifier() and GradientBoostingClassifier() (maxdf=0.75, ngramrange=(1,1)). """def encode_text_tfidf_vectors_test(in_folder_train, in_folder_test, tfidf_max_df, tfidf_ngram_range): # replace this path with your own after you've unzipped the input file.    fileslist_train = os.listdir(in_folder_train)    fileslist_test = os.listdir(in_folder_test)    y_train = []    y_test = []    rawdata_train = []    rawdata_test = []    vectorizer = TfidfVectorizer(ngram_range=tfidf_ngram_range, max_df=tfidf_max_df)    for i in fileslist_train:        namesplited_train = [''.join(g) for _, g in groupby(i, str.isalpha)]        filepath_train = in_folder_train + '/' + i        f_train = open(filepath_train,'r')        lines_train = f_train.readlines()        lines1_train = [x.replace("\n", "") for x in lines_train]        rawdata_train.append(lines1_train[2])        # print(lines1_train[2])        if namesplited_train[1] == 'legit':            y_train.append(1)        else:            y_train.append(0)    for i in fileslist_test:        namesplited_test = [''.join(g) for _, g in groupby(i, str.isalpha)]        filepath_test = in_folder_test + '/' + i        f_test = open(filepath_test,'r')        lines_test = f_test.readlines()        lines1_test = [x.replace("\n", "") for x in lines_test]        rawdata_test.append(lines1_test[2])        # print(lines1_test[2])        if namesplited_test[1] == 'legit':            y_test.append(1)        else:            y_test.append(0)    vectorizer.fit(rawdata_train)    vector_train = vectorizer.transform(rawdata_train)    vector_test = vectorizer.transform(rawdata_test)    vocabulary_train = vectorizer.vocabulary_    IDF = vectorizer.idf_    # print(vocabulary_train)    # print(IDF)    # print("vector_train")    # print(vector_train.shape)    # print(vector_train.toarray())    # print("vector_test")    # print(vector_test.shape)    # print(vector_test.toarray())    dense_train = vector_train.todense()    dense_test = vector_test.todense()    feature_names = vectorizer.get_feature_names()    dense_list = dense_train.tolist()    df = pd.DataFrame(dense_list,columns=feature_names)    X_train = dense_train    X_test = dense_test    # print(df)    # print(len(y))    # put your code here. You can define helper functions, but if you do please define them before this function    return X_train, y_train, X_test, y_testX_train, y_train, X_test, y_test = encode_text_tfidf_vectors_test('/Users/jiadesong/Desktop/ISE540/hw3/files/train', '/Users/jiadesong/Desktop/ISE540/hw3/files/test', 0.75, (1, 1))kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)Tree = tree.DecisionTreeClassifier()Tree.fit(X_train, y_train)pred1 = Tree.predict(X_test)GBC = GradientBoostingClassifier()GBC.fit(X_train, y_train)pred2 = GBC.predict(X_test)print('Tree test accuracy: ')print(sklearn.metrics.accuracy_score(y_test, pred1))print('GBC test accuracy: ')print(sklearn.metrics.accuracy_score(y_test, pred2))print('Ideal value for p: ')print(sum(y_test)/len(y_test))"""The accuracy measures are f1_score for both. The difference are both about 32% classifiers, which are greater than cross validation (15%). The ideal p value is 80%. If we label data randomly, the accuracy rate is: P = (80%)^2 + (20%)^2 = 68%""""""[10 points Extra Credit] Using the subject tf-idf rather than main-body, re-train your classifier/model of choice above (you do not have to try out fifteen different things for this exercise), and apply it to the test set (again, using subject tf-idf as features). Does the performance improve? """def encode_text_tfidf_vectors_test_subject(in_folder_train, in_folder_test, tfidf_max_df, tfidf_ngram_range): # replace this path with your own after you've unzipped the input file.    fileslist_train = os.listdir(in_folder_train)    fileslist_test = os.listdir(in_folder_test)    y_train = []    y_test = []    rawdata_train = []    rawdata_test = []    vectorizer = TfidfVectorizer(ngram_range=tfidf_ngram_range, max_df=tfidf_max_df)    for i in fileslist_train:        namesplited_train = [''.join(g) for _, g in groupby(i, str.isalpha)]        filepath_train = in_folder_train + '/' + i        f_train = open(filepath_train,'r')        lines_train = f_train.readlines()        lines1_train = [x.replace("\n", "") for x in lines_train]        rawdata_train.append(lines1_train[2])        # print(lines1_train[2])        if namesplited_train[1] == 'legit':            y_train.append(1)        else:            y_train.append(0)    for i in fileslist_test:        namesplited_test = [''.join(g) for _, g in groupby(i, str.isalpha)]        filepath_test = in_folder_test + '/' + i        f_test = open(filepath_test,'r')        lines_test = f_test.readlines()        lines1_test = [x.replace("\n", "") for x in lines_test]        rawdata_test.append(lines1_test[0])        # print(lines1_test[2])        if namesplited_test[1] == 'legit':            y_test.append(1)        else:            y_test.append(0)    vectorizer.fit(rawdata_train)    vector_train = vectorizer.transform(rawdata_train)    vector_test = vectorizer.transform(rawdata_test)    vocabulary_train = vectorizer.vocabulary_    IDF = vectorizer.idf_    # print(vocabulary_train)    # print(IDF)    # print("vector_train")    # print(vector_train.shape)    # print(vector_train.toarray())    # print("vector_test")    # print(vector_test.shape)    # print(vector_test.toarray())    dense_train = vector_train.todense()    dense_test = vector_test.todense()    feature_names = vectorizer.get_feature_names()    dense_list = dense_train.tolist()    df = pd.DataFrame(dense_list,columns=feature_names)    X_train = dense_train    X_test = dense_test    # print(df)    # print(len(y))    # put your code here. You can define helper functions, but if you do please define them before this function    return X_train, y_train, X_test, y_testprint(" ")print(" ")print("For the Subject Dataset: ")X_train1, y_train1, X_test1, y_test1 = encode_text_tfidf_vectors_test_subject('/Users/jiadesong/Desktop/ISE540/hw3/files/train', '/Users/jiadesong/Desktop/ISE540/hw3/files/test', 0.75, (1, 1))Tree1 = tree.DecisionTreeClassifier()Tree1.fit(X_train1, y_train1)pred11 = Tree1.predict(X_test1)GBC1 = GradientBoostingClassifier()GBC1.fit(X_train1, y_train1)pred21 = GBC1.predict(X_test1)print('Tree test accuracy: ')print(sklearn.metrics.accuracy_score(y_test1, pred11))print('GBC test accuracy: ')print(sklearn.metrics.accuracy_score(y_test1, pred21))"""The performance improved. (From 68% to 80%)"""